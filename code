{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6920046,"sourceType":"datasetVersion","datasetId":3973543}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jasminemohamed2545/detecting-ai-text-98-svm-nn-full-ml-guide?scriptVersionId=260905140\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Steps of creating an ML model:\n1. Data collection and preparation\n2. Feature engineering and selection\n3. Model selection and architecture\n4. Training and validation\n5. Model evaluation and testing\n6. Deployment and integration\n7. Monitoring and maintenance\n\nGeneral ml modeling workflow:\n1) Understand the Problem\n‚Üí What type of task is it?\n‚Üí Classification (predict categories), Regression (predict continuous values), or Clustering (unsupervised grouping)?\n‚Üí Think about the business goal or real-world impact.\n\n2) Load the textDetection_df\n‚Üí Load from CSV, Excel, SQL, API, etc.\n‚Üí Use libraries like pandas, numpy, or sqlalchemy.\n\n3) EDA (Exploratory Data Analysis)\n‚Üí Goal: Understand your data before modeling.\nData Types Check: Are features numeric, categorical, datetime, text?\nMissing Values: Count them, visualize them (textDetection_df.isnull().sum(), heatmaps).\nBasic Statistics: Mean, median, mode, std, min, max, percentiles.\nValue Counts: For categorical columns.\nClass Balance: For classification ‚Äî Is the target imbalanced? (e.g. 90% vs 10%)\nCorrelations: Pearson/Spearman matrix, heatmap ‚Üí check for collinearity.\nOutliers: Boxplots, z-score, IQR method to detect outliers.\nDistributions: Histograms, KDE plots to understand variable distribution.\nRelationships: Scatter plots, pairplots (like from seaborn) for bivariate/multivariate insights.\n\n4) Preprocessing\n‚Üí Prepare your data for ML algorithms.\nMissing Value Handling: Drop rows, fill with mean/median/mode or use imputation.\nEncoding: One-Hot Encoding (OHE), Label Encoding for categoricals.\nScaling: StandardScaler, MinMaxScaler ‚Äî esp. important for distance-based models (e.g., KNN, SVM).\nText Preprocessing: Tokenization, stopwords removal, TF-ItextDetection_df, etc.\nDatetime Features: Extract day, month, weekday, hour, etc.\nFeature Engineering: Create new features from existing ones (e.g., ratios, interactions).\nDimensionality Reduction: PCA, t-SNE for visualization or reducing noise.\n\n5) Split Data\n‚Üí Split your dataset into training, validation, and test sets.\n‚Üí Common: 80% train / 20% test, or use train_test_split() from sklearn.\n‚Üí Use Stratified Split for classification if class imbalance exists.\n‚Üí Optionally use Cross-Validation (CV) for more reliable evaluation.\n\n6) Model Selection\n‚Üí Pick models based on the task & data.\nClassification Models: Logistic Regression, Decision Tree, Random Forest, SVM, KNN, Naive Bayes, XGBoost, LightGBM, Neural Networks.\nRegression Models: Linear Regression, Ridge, Lasso, Decision Tree Regressor, SVR, XGBoost Regressor.\nClustering Models: KMeans, DBSCAN, Agglomerative Clustering.\n\n7) Train the Model\n‚Üí Fit your model on the training data using .fit()\n‚Üí Watch for overfitting or underfitting.\n\n8) Evaluate the Model\n‚Üí Depends on the problem type:\n* _-Classification Metrics:\nAccuracy: Overall correct predictions.\nPrecision: True Positives / (True Positives + False Positives)\nRecall: True Positives / (True Positives + False Negatives)\nF1 Score: Harmonic mean of Precision & Recall.\nConfusion Matrix: TP, FP, TN, FN ‚Üí visualize errors.\nROC AUC: Performance across different thresholds.\nPR AUC: When dealing with imbalanced classes.\n* -Regression Metrics:\nMSE (Mean Squared Error)\nRMSE (Root Mean Squared Error)\nMAE (Mean Absolute Error)\nR¬≤ Score: Percentage of variance explained.\n* -Visualization Tools:\nROC Curve, Precision-Recall Curve\nResidual plots, prediction error plots","metadata":{}},{"cell_type":"markdown","source":"# 1) Understanding the Problem --> Classification task ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report , ConfusionMatrixDisplay \n\n\nfilePath =\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\" \ntextDetection_df =pd.read_csv(filePath)\ntextDetection_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:32:52.200286Z","iopub.execute_input":"2025-09-07T23:32:52.200681Z","iopub.status.idle":"2025-09-07T23:32:53.208341Z","shell.execute_reply.started":"2025-09-07T23:32:52.200654Z","shell.execute_reply":"2025-09-07T23:32:53.207206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2) EDA-->Data Types Check, Missing Values, Basic Stats, Value Counts, Class Balance ‚Äì For classification, Correlations, Outliers, Visualization, distributions, relationships","metadata":{}},{"cell_type":"code","source":"print(f\"Column Names:\\n{textDetection_df.columns}\")\nprint('-' * 60)\nprint(f\"Shape of DataFrame (rows, columns): {textDetection_df.shape}\") # Rows and Columns\nprint('-' * 60)\nprint(\"Data Types and Non-null Counts:\")\nprint(textDetection_df.info()) # Data Types Check\nprint('-' * 60)\nprint(f'no of nulls is: {textDetection_df.isnull().sum().sum()}') # Missing Values\nprint(f'no of duplicates: {textDetection_df.duplicated().sum()}') # Duplicates\nprint('-' * 60)\nprint(\"Basic Statistical Summary for Numerical Columns:\")\nprint(textDetection_df.describe()) # Basic Stats\nprint('-' * 60)\nprint(\"Missing Values Per Column:\")\nprint(textDetection_df.isna().sum()) # Missing Values\n\ncategorical_cols = textDetection_df.select_dtypes(include='object')\n\n##class balance would refer to whether the dataset contains equal or unequal numbers of samples for each class. for Example the value counts of the target variable in a classification problem. If the dataset is imbalanced, it may affect the performance of the model.\nfor col in categorical_cols:\n    print(f'\\nValue Counts for {col}:')\n    print(textDetection_df[col].value_counts()) # Value Counts  shows how many times each unique value appears in a column categorical column mostly for classification problems only or a single column only at each time\n\nprint(f\"Value Counts for Target Column 'generated':\")\nprint(textDetection_df['generated'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:32:53.209749Z","iopub.execute_input":"2025-09-07T23:32:53.210179Z","iopub.status.idle":"2025-09-07T23:32:53.47981Z","shell.execute_reply.started":"2025-09-07T23:32:53.210154Z","shell.execute_reply":"2025-09-07T23:32:53.478543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizations","metadata":{}},{"cell_type":"markdown","source":"## 1. Visualizing class imbalances using countplot","metadata":{}},{"cell_type":"code","source":"# Visualize the class balance\nsns.countplot(x='generated', data=textDetection_df, color='purple')\nplt.title('Class Balance')  \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:32:53.480573Z","iopub.execute_input":"2025-09-07T23:32:53.480969Z","iopub.status.idle":"2025-09-07T23:32:53.627987Z","shell.execute_reply.started":"2025-09-07T23:32:53.480923Z","shell.execute_reply":"2025-09-07T23:32:53.62706Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Displaying Word Clouds for Each Class ‚òÅÔ∏è\nThis code will generate two images side-by-side: one for the most common words in student essays and one for AI-generated essays.","metadata":{}},{"cell_type":"code","source":"# Word Clouds for Each Class\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Step 1: Separate the text data into two groups\nstudent_text = ' '.join(textDetection_df[textDetection_df['generated'] == 0]['text'])\n\nai_text = ' '.join(textDetection_df[textDetection_df['generated'] == 1]['text'])\n\n\n# Step 2: Create the WordCloud objects\nwordcloud_student = WordCloud(width=800, height=400, background_color='white').generate(student_text)\nwordcloud_ai = WordCloud(width=800, height=400, background_color='black').generate(ai_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:32:53.629659Z","iopub.execute_input":"2025-09-07T23:32:53.629972Z","iopub.status.idle":"2025-09-07T23:33:21.607425Z","shell.execute_reply.started":"2025-09-07T23:32:53.62995Z","shell.execute_reply":"2025-09-07T23:33:21.606074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Display the generated images using matplotlib \nplt.figure(figsize=(20, 10))\n\n# Display student word cloud in the first subplot\nplt.imshow(wordcloud_student, interpolation='bilinear')\nplt.title('Most Common Words in Student-Written Essays', fontsize=16)\nplt.axis('off') # Hide the axes\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:21.609002Z","iopub.execute_input":"2025-09-07T23:33:21.609919Z","iopub.status.idle":"2025-09-07T23:33:22.261435Z","shell.execute_reply.started":"2025-09-07T23:33:21.609871Z","shell.execute_reply":"2025-09-07T23:33:22.260374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\n# Display AI word cloud in the second subplot\nplt.imshow(wordcloud_ai, interpolation='bilinear')\nplt.title('Most Common Words in AI-Generated Essays', fontsize=16)\nplt.axis('off') # Hide the axes\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:22.26254Z","iopub.execute_input":"2025-09-07T23:33:22.262829Z","iopub.status.idle":"2025-09-07T23:33:22.899048Z","shell.execute_reply.started":"2025-09-07T23:33:22.262807Z","shell.execute_reply":"2025-09-07T23:33:22.897961Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Dislaying Distribution of Essay Lengths üìè\nThis code first creates a new column word_count and then visualizes its distribution for both classes using a histogram.","metadata":{}},{"cell_type":"code","source":"# Step 1: Create a new column for word count \n# This calculates the number of words in each essay\ntextDetection_df['word_count'] = textDetection_df['text'].str.split().str.len()\n\n\n# Step 2: Visualize the distribution using a histogram\nplt.figure(figsize=(12, 7))\n\n# Use seaborn's histplot to compare the distributions\n# The 'hue' parameter automatically creates separate histograms for each class\nsns.histplot(data=textDetection_df, x='word_count', hue='generated', kde=True, element='step')\n\nplt.title('Distribution of Essay Word Count (Student vs. AI)', fontsize=16)\nplt.xlabel('Word Count', fontsize=12)\nplt.ylabel('Number of Essays', fontsize=12)\nplt.legend(title='Generated', labels=['AI (1)', 'Student (0)'])\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:22.899983Z","iopub.execute_input":"2025-09-07T23:33:22.900286Z","iopub.status.idle":"2025-09-07T23:33:25.195226Z","shell.execute_reply.started":"2025-09-07T23:33:22.900264Z","shell.execute_reply":"2025-09-07T23:33:25.194156Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"#3) Preprocessing: Handle missing values, encoding, scaling, text processing, datetime features, feature engineering, dimensionality reduction\n Handle missing values: Drop or fill them\n Step 1: Identify missing values done in the preceding step\n\n Step 2: Evaluate if we should drop or impute\n in our case the id for the text has no much importance but the text each holds is what we care abt, so i will fill the ids with medians but for null text it will be dropped\n\nstep 3: Drop or fill missing values\nstep 4: Drop duplicates based on 'text' column\nstep 5: Encoding 'text' column to get features","metadata":{}},{"cell_type":"code","source":"# Drop rows where 'text' is null\ntextDetection_df = textDetection_df.dropna(subset=['text'])\n\n# Drop empty texts like \"    \"\ntextDetection_df['text'] = textDetection_df['text'].astype(str)\ntextDetection_df = textDetection_df[textDetection_df['text'].str.strip().astype(bool)]\n\n# Drop duplicates based on text\ntextDetection_df = textDetection_df.drop_duplicates(subset=['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:25.196207Z","iopub.execute_input":"2025-09-07T23:33:25.196685Z","iopub.status.idle":"2025-09-07T23:33:25.247942Z","shell.execute_reply.started":"2025-09-07T23:33:25.196646Z","shell.execute_reply":"2025-09-07T23:33:25.246944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Checking after preprossesing","metadata":{}},{"cell_type":"code","source":"print(f'no of nulls is: {textDetection_df.isnull().sum().sum()}') # Missing Values\nprint(f'no of duplicates: {textDetection_df.duplicated().sum()}') # Duplicates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:25.249013Z","iopub.execute_input":"2025-09-07T23:33:25.249314Z","iopub.status.idle":"2025-09-07T23:33:25.412165Z","shell.execute_reply.started":"2025-09-07T23:33:25.249292Z","shell.execute_reply":"2025-09-07T23:33:25.4111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(textDetection_df['text'].head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:25.416199Z","iopub.execute_input":"2025-09-07T23:33:25.416499Z","iopub.status.idle":"2025-09-07T23:33:25.423259Z","shell.execute_reply.started":"2025-09-07T23:33:25.416478Z","shell.execute_reply":"2025-09-07T23:33:25.422149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Till now we have no features to corr with the output or predict it\nsince text is unstructured ‚Üí we need to extract features from it., so we will encode the text column with  TF-IDF\nthrough steps:\n1) importing a tool called TfidfVectorizer from scikit-learn\n2) TF-IDF stands for Term Frequency-Inverse Document Frequency.\n3) It converts text into numbers based on how important words are in each document (essay).\n4)  creating a TF-IDF vectorizer object.\n5) max_features=1000: Keep only the top 1000 most important words.\n6) stop_words='english': Ignore very common English words (like the, is, and).\n7) `X = tfidf.fit_transform(df['text']) -->` Goes through every essay in df['text'] and Builds a list of important words across all essays Then, for each essay, it creates a row of numbers ‚Äî how much each word matters in that essay Ex: Essay 1: \"Cars are fast and useful.\"\n‚Üí Might become [0.6, 0.4, 0.5, 0, 0, 0]\n8) X is now a feature matrix\nX.shape:  (number of essays, number of words), each row is one essay and each column is one word\n9) TF-IDF know identifies words as important:\n    Term Frequency (TF): How often a word appears in one essay, more appearance = more importance.\n    Inverse Document Frequency (IDF): How rare that word is across all essays, 'the' is the least rare so least score.\n10) And then the model will identify an essay as ai generated or not through:\n    The model doesn‚Äôt \"understand\" meaning like a human.\n    But it learns patterns in word usage EX: Students often write: \"I think...\", \"in conclusion...\", \"my opinion is...\"\n    AI often writes: \"This essay aims to...\", \"one might argue that...\", \"in recent years...\"\n    Even if both sound good, their word patterns are different.\n    TF-IDF captures those patterns as numbers.\n    Your model (like SVM or a neural net) looks at those numbers and learns the difference from training data.\n    It doesn‚Äôt know ‚Äúwhy‚Äù ‚Äî it just sees that essays with pattern A ‚Üí usually label 0 (student),\n    and essays with pattern B ‚Üí usually label 1 (AI).\n","metadata":{}},{"cell_type":"markdown","source":"Splitting the DS","metadata":{}},{"cell_type":"code","source":"#clean the text data to prevent using irrelevant words or nonmeaningful ones\nimport re\n\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove numbers and punctuation\n    text = re.sub(r'[^a-z\\s]', '', text)\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\nx = textDetection_df['text']\ny = textDetection_df['generated']\n# Splitting the dataset into training and testing sets\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n# Apply cleaning to all text entries\n# Convert x_train and x_test to DataFrames to add a new column\nx_train = x_train.to_frame()\nx_test = x_test.to_frame()\n\n# Apply cleaning to all text entries\nx_train['clean_text'] = x_train['text'].apply(clean_text)\nx_test['clean_text'] = x_test['text'].apply(clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:25.424239Z","iopub.execute_input":"2025-09-07T23:33:25.424542Z","iopub.status.idle":"2025-09-07T23:33:30.534576Z","shell.execute_reply.started":"2025-09-07T23:33:25.42452Z","shell.execute_reply":"2025-09-07T23:33:30.533274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Getting features","metadata":{}},{"cell_type":"code","source":"# Converting text to features using TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ntfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', min_df=5, max_df=0.9)# only keep words that appear in 5+ docs & ignore words that appear in 90%+ of docs\nfeatures_from_text_train = tfidf_vectorizer.fit_transform(x_train['clean_text']).toarray()\nfeatures_from_text_test = tfidf_vectorizer.transform(x_test['clean_text']).toarray()#features_from_text is now a feature matrix\n#features_from_text.shape:  (number of essays, number of words), each row is one essay and each column is one word\n\nprint(\"Non-zero values:\", np.count_nonzero(features_from_text_train))\nprint(\"Shape:\", features_from_text_train.shape)\n# After fitting the vectorizer\nfeature_names = tfidf_vectorizer.get_feature_names_out()\nprint(feature_names[:100])  # print first 20 words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:30.535462Z","iopub.execute_input":"2025-09-07T23:33:30.535804Z","iopub.status.idle":"2025-09-07T23:33:37.417765Z","shell.execute_reply.started":"2025-09-07T23:33:30.535775Z","shell.execute_reply":"2025-09-07T23:33:37.416669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_from_text_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:37.418697Z","iopub.execute_input":"2025-09-07T23:33:37.41897Z","iopub.status.idle":"2025-09-07T23:33:37.425937Z","shell.execute_reply.started":"2025-09-07T23:33:37.418949Z","shell.execute_reply":"2025-09-07T23:33:37.424964Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Scaling through standardscaler & Feature Reduction with PCA:\n","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nx_traScal = scaler.fit_transform(features_from_text_train)\nx_tesScal = scaler.transform(features_from_text_test)\n#########################################################################################\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=100)\nx_tra_pca = pca.fit_transform(x_traScal)\nx_tes_pca = pca.transform(x_tesScal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:37.427Z","iopub.execute_input":"2025-09-07T23:33:37.427343Z","iopub.status.idle":"2025-09-07T23:33:40.220891Z","shell.execute_reply.started":"2025-09-07T23:33:37.427314Z","shell.execute_reply":"2025-09-07T23:33:40.219917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Showing diff correlations bet data:","metadata":{}},{"cell_type":"code","source":"#converting features_from_text matrix to a DataFrame to see each word and its corresponding TF-IDF value score of importance\nimport pandas as pd\n\nfeatures_from_textdf = pd.DataFrame(features_from_text_train, columns=feature_names)\nprint(features_from_textdf.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:40.221931Z","iopub.execute_input":"2025-09-07T23:33:40.222208Z","iopub.status.idle":"2025-09-07T23:33:40.237527Z","shell.execute_reply.started":"2025-09-07T23:33:40.222188Z","shell.execute_reply":"2025-09-07T23:33:40.236491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Showing correlation between the features' words\ntop10words = features_from_textdf.mean().sort_values(ascending=False).head(10).index.tolist()\ncorrelation_matrix1 = features_from_textdf[top10words].corr()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix1, cmap='coolwarm', annot=True)\nplt.title(\"Correlation Between TF-IDF Features\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:40.238587Z","iopub.execute_input":"2025-09-07T23:33:40.238893Z","iopub.status.idle":"2025-09-07T23:33:40.817959Z","shell.execute_reply.started":"2025-09-07T23:33:40.238844Z","shell.execute_reply":"2025-09-07T23:33:40.817035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Displaying Top Important Words by Average TF-IDF Score","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\ntop_20_words = features_from_textdf.mean().sort_values(ascending=False).head(20)\n\nfig = px.bar(\n    x=top_20_words.values,\n    y=top_20_words.index,\n    labels={'x': 'Avg TF-IDF Score', 'y': 'Word'},\n    title='Top 20 Most Important Words (by Average TF-IDF Score)',\n    color=top_20_words.values,\n    color_continuous_scale='Viridis'\n)\n\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:34:45.190287Z","iopub.execute_input":"2025-09-07T23:34:45.190551Z","iopub.status.idle":"2025-09-07T23:34:45.33595Z","shell.execute_reply.started":"2025-09-07T23:34:45.190531Z","shell.execute_reply":"2025-09-07T23:34:45.33464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SVM Models","metadata":{}},{"cell_type":"markdown","source":"4) Modeling\n A-Using SVM for classification + model evaluation","metadata":{}},{"cell_type":"code","source":"svm = { 'linear svm' : SVC(kernel='linear') ,\n       'svc (RBF kernel)': SVC(kernel='rbf') }\n\nprint('# SVM MODELS # \\n')\nfor name , model in svm.items():\n    model.fit(x_tra_pca,y_train)\n    y_pred = model.predict(x_tes_pca)\n    print('Accuracy of model ' , name , accuracy_score(y_test,y_pred) , '\\n')\n    print(classification_report(y_test,y_pred))\n    print('########################################################\\n')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:34:13.058212Z","iopub.execute_input":"2025-09-07T23:34:13.058541Z","iopub.status.idle":"2025-09-07T23:34:45.188696Z","shell.execute_reply.started":"2025-09-07T23:34:13.058522Z","shell.execute_reply":"2025-09-07T23:34:45.187354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"poly_svm = SVC(kernel='poly', degree=3)\npoly_svm.fit(x_tra_pca, y_train)\n\n#svm polynomial model\ny_pred_poly = poly_svm.predict(x_tes_pca)\nprint('Accuracy of model ' , 'Polynomial SVM' , accuracy_score(y_test,y_pred_poly) , '\\n')\nprint(classification_report(y_test, y_pred_poly))\nprint('########################################################\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:35:11.995494Z","iopub.execute_input":"2025-09-07T23:35:11.995795Z","iopub.status.idle":"2025-09-07T23:35:20.533728Z","shell.execute_reply.started":"2025-09-07T23:35:11.995774Z","shell.execute_reply":"2025-09-07T23:35:20.532396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lin_model = SVC(kernel='linear')\nlin_model.fit(x_tra_pca,y_train)\ny_pred_lin = lin_model.predict(x_tes_pca)\n\nconf_lin = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_lin))   \nconf_lin.plot()\nplt.title('Confusion Matrix for Linear SVM')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:35:20.535364Z","iopub.execute_input":"2025-09-07T23:35:20.535658Z","iopub.status.idle":"2025-09-07T23:35:45.110061Z","shell.execute_reply.started":"2025-09-07T23:35:20.535631Z","shell.execute_reply":"2025-09-07T23:35:45.109025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_poly = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_poly))\nconf_poly.plot()\nplt.title('Confusion Matrix for Polynomial SVM')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:35:45.111162Z","iopub.execute_input":"2025-09-07T23:35:45.111416Z","iopub.status.idle":"2025-09-07T23:35:45.435329Z","shell.execute_reply.started":"2025-09-07T23:35:45.111398Z","shell.execute_reply":"2025-09-07T23:35:45.434159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RBF_model = SVC(kernel='rbf')\nRBF_model.fit(x_tra_pca,y_train)\ny_pred_RBF = RBF_model.predict(x_tes_pca)\n\nconf_RBF = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_RBF))\nconf_RBF.plot()\nplt.title('Confusion Matrix for RBF SVM')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:35:45.436777Z","iopub.execute_input":"2025-09-07T23:35:45.437068Z","iopub.status.idle":"2025-09-07T23:35:51.743337Z","shell.execute_reply.started":"2025-09-07T23:35:45.437046Z","shell.execute_reply":"2025-09-07T23:35:51.742248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neural Networks Model","metadata":{}},{"cell_type":"markdown","source":"Neural Network Implementation ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport joblib\n\n# Step 1: Define hyperparameter grid\nparam_grid = {\n    'hidden_layer_sizes': [(10,), (50,), (64, 32), (64, 32, 16)],\n    'activation': ['relu', 'logistic'],\n    'alpha': [0.0001, 0.001, 0.01],\n    'learning_rate_init': [0.001, 0.01]\n}\n\n# Step 2: Define base model\nbase_mlp = MLPClassifier(max_iter=200, early_stopping=True, random_state=42)\n\n# Step 3: Grid search\ngrid_search = GridSearchCV(\n    base_mlp, param_grid, cv=5,\n    n_jobs=-1, verbose=1, scoring='accuracy'\n)\ngrid_search.fit(x_tra_pca, y_train)\n\n# Step 4: Get the best model\nbest_model = grid_search.best_estimator_\nprint(\"\\n Best Hyperparameters:\")\nprint(grid_search.best_params_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:35:54.750807Z","iopub.execute_input":"2025-09-07T23:35:54.751167Z","iopub.status.idle":"2025-09-07T23:39:51.428317Z","shell.execute_reply.started":"2025-09-07T23:35:54.751141Z","shell.execute_reply":"2025-09-07T23:39:51.427458Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"the Neural Network Loss Curve, shows how the loss (error) of your model changes over training epochs (iterations).","metadata":{}},{"cell_type":"code","source":"# Step 5: Evaluate on test set\ny_predNN = best_model.predict(x_tes_pca)\nacc = accuracy_score(y_test, y_predNN)\nprint(f\"\\nTest Accuracy: {acc:.4f}\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_predNN))\n\n# Step 6: Cross-validation on training set\ncv_scores = cross_val_score(best_model, x_tra_pca, y_train, cv=5, scoring='accuracy')\nprint(f\"Cross-validation scores (5-fold): {cv_scores}\")\nprint(f\"Mean CV Accuracy: {cv_scores.mean():.4f}\")\n\n# Step 7: Plot loss curve\nplt.plot(best_model.loss_curve_)\nplt.title(\"Neural Network Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.grid(True)\nplt.show()\n\n# Step 8: Save the best model\njoblib.dump(best_model, \"best_nn_model.pkl\")\nprint(\" Best model saved as 'best_nn_model.pkl'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:39:57.65087Z","iopub.execute_input":"2025-09-07T23:39:57.651239Z","iopub.status.idle":"2025-09-07T23:40:03.884849Z","shell.execute_reply.started":"2025-09-07T23:39:57.651206Z","shell.execute_reply":"2025-09-07T23:40:03.883794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_nn = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_predNN))   \nconf_nn.plot()\nplt.title('Confusion Matrix for NN')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:43:01.427378Z","iopub.execute_input":"2025-09-07T23:43:01.427699Z","iopub.status.idle":"2025-09-07T23:43:01.601199Z","shell.execute_reply.started":"2025-09-07T23:43:01.427676Z","shell.execute_reply":"2025-09-07T23:43:01.600245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Evaluation","metadata":{}},{"cell_type":"code","source":"print(\"Model Comparison Summary:\")\nsummary_data = {\n    \"Model\": [\"SVM-Linear\", \"SVM-RBF\", \"SVM-Poly\", \"Neural Net (Best)\"],\n    \"Test Accuracy\": [accuracy_score(y_test, y_pred_lin),\n                      accuracy_score(y_test, y_pred_RBF),\n                      accuracy_score(y_test, y_pred_poly),\n                      accuracy_score(y_test, y_predNN)],\n}\nsummary_df = pd.DataFrame(summary_data)\nprint(summary_df)\nbest_model_name = summary_df.loc[summary_df['Test Accuracy'].idxmax(), 'Model']\nprint(f'Best Model is: {best_model_name}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:43:04.750975Z","iopub.execute_input":"2025-09-07T23:43:04.751317Z","iopub.status.idle":"2025-09-07T23:43:04.767068Z","shell.execute_reply.started":"2025-09-07T23:43:04.751263Z","shell.execute_reply":"2025-09-07T23:43:04.766052Z"}},"outputs":[],"execution_count":null}]}