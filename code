{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6920046,"sourceType":"datasetVersion","datasetId":3973543}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jasminemohamed2545/detecting-ai-text-98-svm-nn-full-ml-guide?scriptVersionId=260905140\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Steps of creating an ML model:\n1. Data collection and preparation\n2. Feature engineering and selection\n3. Model selection and architecture\n4. Training and validation\n5. Model evaluation and testing\n6. Deployment and integration\n7. Monitoring and maintenance\n\nGeneral ml modeling workflow:\n1) Understand the Problem\n→ What type of task is it?\n→ Classification (predict categories), Regression (predict continuous values), or Clustering (unsupervised grouping)?\n→ Think about the business goal or real-world impact.\n\n2) Load the textDetection_df\n→ Load from CSV, Excel, SQL, API, etc.\n→ Use libraries like pandas, numpy, or sqlalchemy.\n\n3) EDA (Exploratory Data Analysis)\n→ Goal: Understand your data before modeling.\nData Types Check: Are features numeric, categorical, datetime, text?\nMissing Values: Count them, visualize them (textDetection_df.isnull().sum(), heatmaps).\nBasic Statistics: Mean, median, mode, std, min, max, percentiles.\nValue Counts: For categorical columns.\nClass Balance: For classification — Is the target imbalanced? (e.g. 90% vs 10%)\nCorrelations: Pearson/Spearman matrix, heatmap → check for collinearity.\nOutliers: Boxplots, z-score, IQR method to detect outliers.\nDistributions: Histograms, KDE plots to understand variable distribution.\nRelationships: Scatter plots, pairplots (like from seaborn) for bivariate/multivariate insights.\n\n4) Preprocessing\n→ Prepare your data for ML algorithms.\nMissing Value Handling: Drop rows, fill with mean/median/mode or use imputation.\nEncoding: One-Hot Encoding (OHE), Label Encoding for categoricals.\nScaling: StandardScaler, MinMaxScaler — esp. important for distance-based models (e.g., KNN, SVM).\nText Preprocessing: Tokenization, stopwords removal, TF-ItextDetection_df, etc.\nDatetime Features: Extract day, month, weekday, hour, etc.\nFeature Engineering: Create new features from existing ones (e.g., ratios, interactions).\nDimensionality Reduction: PCA, t-SNE for visualization or reducing noise.\n\n5) Split Data\n→ Split your dataset into training, validation, and test sets.\n→ Common: 80% train / 20% test, or use train_test_split() from sklearn.\n→ Use Stratified Split for classification if class imbalance exists.\n→ Optionally use Cross-Validation (CV) for more reliable evaluation.\n\n6) Model Selection\n→ Pick models based on the task & data.\nClassification Models: Logistic Regression, Decision Tree, Random Forest, SVM, KNN, Naive Bayes, XGBoost, LightGBM, Neural Networks.\nRegression Models: Linear Regression, Ridge, Lasso, Decision Tree Regressor, SVR, XGBoost Regressor.\nClustering Models: KMeans, DBSCAN, Agglomerative Clustering.\n\n7) Train the Model\n→ Fit your model on the training data using .fit()\n→ Watch for overfitting or underfitting.\n\n8) Evaluate the Model\n→ Depends on the problem type:\n* _-Classification Metrics:\nAccuracy: Overall correct predictions.\nPrecision: True Positives / (True Positives + False Positives)\nRecall: True Positives / (True Positives + False Negatives)\nF1 Score: Harmonic mean of Precision & Recall.\nConfusion Matrix: TP, FP, TN, FN → visualize errors.\nROC AUC: Performance across different thresholds.\nPR AUC: When dealing with imbalanced classes.\n* -Regression Metrics:\nMSE (Mean Squared Error)\nRMSE (Root Mean Squared Error)\nMAE (Mean Absolute Error)\nR² Score: Percentage of variance explained.\n* -Visualization Tools:\nROC Curve, Precision-Recall Curve\nResidual plots, prediction error plots","metadata":{}},{"cell_type":"markdown","source":"# 1) Understanding the Problem --> Classification task ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report , ConfusionMatrixDisplay \n\n\nfilePath =\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\" \ntextDetection_df =pd.read_csv(filePath)\ntextDetection_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:32:52.200286Z","iopub.execute_input":"2025-09-07T23:32:52.200681Z","iopub.status.idle":"2025-09-07T23:32:53.208341Z","shell.execute_reply.started":"2025-09-07T23:32:52.200654Z","shell.execute_reply":"2025-09-07T23:32:53.207206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2) EDA-->Data Types Check, Missing Values, Basic Stats, Value Counts, Class Balance – For classification, Correlations, Outliers, Visualization, distributions, relationships","metadata":{}},{"cell_type":"code","source":"print(f\"Column Names:\\n{textDetection_df.columns}\")\nprint('-' * 60)\nprint(f\"Shape of DataFrame (rows, columns): {textDetection_df.shape}\") # Rows and Columns\nprint('-' * 60)\nprint(\"Data Types and Non-null Counts:\")\nprint(textDetection_df.info()) # Data Types Check\nprint('-' * 60)\nprint(f'no of nulls is: {textDetection_df.isnull().sum().sum()}') # Missing Values\nprint(f'no of duplicates: {textDetection_df.duplicated().sum()}') # Duplicates\nprint('-' * 60)\nprint(\"Basic Statistical Summary for Numerical Columns:\")\nprint(textDetection_df.describe()) # Basic Stats\nprint('-' * 60)\nprint(\"Missing Values Per Column:\")\nprint(textDetection_df.isna().sum()) # Missing Values\n\ncategorical_cols = textDetection_df.select_dtypes(include='object')\n\n##class balance would refer to whether the dataset contains equal or unequal numbers of samples for each class. for Example the value counts of the target variable in a classification problem. If the dataset is imbalanced, it may affect the performance of the model.\nfor col in categorical_cols:\n    print(f'\\nValue Counts for {col}:')\n    print(textDetection_df[col].value_counts()) # Value Counts  shows how many times each unique value appears in a column categorical column mostly for classification problems only or a single column only at each time\n\nprint(f\"Value Counts for Target Column 'generated':\")\nprint(textDetection_df['generated'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:32:53.209749Z","iopub.execute_input":"2025-09-07T23:32:53.210179Z","iopub.status.idle":"2025-09-07T23:32:53.47981Z","shell.execute_reply.started":"2025-09-07T23:32:53.210154Z","shell.execute_reply":"2025-09-07T23:32:53.478543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizations","metadata":{}},{"cell_type":"markdown","source":"## 1. Visualizing class imbalances using countplot","metadata":{}},{"cell_type":"code","source":"# Visualize the class balance\nsns.countplot(x='generated', data=textDetection_df, color='purple')\nplt.title('Class Balance')  \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:32:53.480573Z","iopub.execute_input":"2025-09-07T23:32:53.480969Z","iopub.status.idle":"2025-09-07T23:32:53.627987Z","shell.execute_reply.started":"2025-09-07T23:32:53.480923Z","shell.execute_reply":"2025-09-07T23:32:53.62706Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Displaying Word Clouds for Each Class ☁️\nThis code will generate two images side-by-side: one for the most common words in student essays and one for AI-generated essays.","metadata":{}},{"cell_type":"code","source":"# Word Clouds for Each Class\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Step 1: Separate the text data into two groups\nstudent_text = ' '.join(textDetection_df[textDetection_df['generated'] == 0]['text'])\n\nai_text = ' '.join(textDetection_df[textDetection_df['generated'] == 1]['text'])\n\n\n# Step 2: Create the WordCloud objects\nwordcloud_student = WordCloud(width=800, height=400, background_color='white').generate(student_text)\nwordcloud_ai = WordCloud(width=800, height=400, background_color='black').generate(ai_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:32:53.629659Z","iopub.execute_input":"2025-09-07T23:32:53.629972Z","iopub.status.idle":"2025-09-07T23:33:21.607425Z","shell.execute_reply.started":"2025-09-07T23:32:53.62995Z","shell.execute_reply":"2025-09-07T23:33:21.606074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Display the generated images using matplotlib \nplt.figure(figsize=(20, 10))\n\n# Display student word cloud in the first subplot\nplt.imshow(wordcloud_student, interpolation='bilinear')\nplt.title('Most Common Words in Student-Written Essays', fontsize=16)\nplt.axis('off') # Hide the axes\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:21.609002Z","iopub.execute_input":"2025-09-07T23:33:21.609919Z","iopub.status.idle":"2025-09-07T23:33:22.261435Z","shell.execute_reply.started":"2025-09-07T23:33:21.609871Z","shell.execute_reply":"2025-09-07T23:33:22.260374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\n# Display AI word cloud in the second subplot\nplt.imshow(wordcloud_ai, interpolation='bilinear')\nplt.title('Most Common Words in AI-Generated Essays', fontsize=16)\nplt.axis('off') # Hide the axes\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:22.26254Z","iopub.execute_input":"2025-09-07T23:33:22.262829Z","iopub.status.idle":"2025-09-07T23:33:22.899048Z","shell.execute_reply.started":"2025-09-07T23:33:22.262807Z","shell.execute_reply":"2025-09-07T23:33:22.897961Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Dislaying Distribution of Essay Lengths 📏\nThis code first creates a new column word_count and then visualizes its distribution for both classes using a histogram.","metadata":{}},{"cell_type":"code","source":"# Step 1: Create a new column for word count \n# This calculates the number of words in each essay\ntextDetection_df['word_count'] = textDetection_df['text'].str.split().str.len()\n\n\n# Step 2: Visualize the distribution using a histogram\nplt.figure(figsize=(12, 7))\n\n# Use seaborn's histplot to compare the distributions\n# The 'hue' parameter automatically creates separate histograms for each class\nsns.histplot(data=textDetection_df, x='word_count', hue='generated', kde=True, element='step')\n\nplt.title('Distribution of Essay Word Count (Student vs. AI)', fontsize=16)\nplt.xlabel('Word Count', fontsize=12)\nplt.ylabel('Number of Essays', fontsize=12)\nplt.legend(title='Generated', labels=['AI (1)', 'Student (0)'])\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:22.899983Z","iopub.execute_input":"2025-09-07T23:33:22.900286Z","iopub.status.idle":"2025-09-07T23:33:25.195226Z","shell.execute_reply.started":"2025-09-07T23:33:22.900264Z","shell.execute_reply":"2025-09-07T23:33:25.194156Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"#3) Preprocessing: Handle missing values, encoding, scaling, text processing, datetime features, feature engineering, dimensionality reduction\n Handle missing values: Drop or fill them\n Step 1: Identify missing values done in the preceding step\n\n Step 2: Evaluate if we should drop or impute\n in our case the id for the text has no much importance but the text each holds is what we care abt, so i will fill the ids with medians but for null text it will be dropped\n\nstep 3: Drop or fill missing values\nstep 4: Drop duplicates based on 'text' column\nstep 5: Encoding 'text' column to get features","metadata":{}},{"cell_type":"code","source":"# Drop rows where 'text' is null\ntextDetection_df = textDetection_df.dropna(subset=['text'])\n\n# Drop empty texts like \"    \"\ntextDetection_df['text'] = textDetection_df['text'].astype(str)\ntextDetection_df = textDetection_df[textDetection_df['text'].str.strip().astype(bool)]\n\n# Drop duplicates based on text\ntextDetection_df = textDetection_df.drop_duplicates(subset=['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:25.196207Z","iopub.execute_input":"2025-09-07T23:33:25.196685Z","iopub.status.idle":"2025-09-07T23:33:25.247942Z","shell.execute_reply.started":"2025-09-07T23:33:25.196646Z","shell.execute_reply":"2025-09-07T23:33:25.246944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Checking after preprossesing","metadata":{}},{"cell_type":"code","source":"print(f'no of nulls is: {textDetection_df.isnull().sum().sum()}') # Missing Values\nprint(f'no of duplicates: {textDetection_df.duplicated().sum()}') # Duplicates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:25.249013Z","iopub.execute_input":"2025-09-07T23:33:25.249314Z","iopub.status.idle":"2025-09-07T23:33:25.412165Z","shell.execute_reply.started":"2025-09-07T23:33:25.249292Z","shell.execute_reply":"2025-09-07T23:33:25.4111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(textDetection_df['text'].head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:25.416199Z","iopub.execute_input":"2025-09-07T23:33:25.416499Z","iopub.status.idle":"2025-09-07T23:33:25.423259Z","shell.execute_reply.started":"2025-09-07T23:33:25.416478Z","shell.execute_reply":"2025-09-07T23:33:25.422149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Till now we have no features to corr with the output or predict it\nsince text is unstructured → we need to extract features from it., so we will encode the text column with  TF-IDF\nthrough steps:\n1) importing a tool called TfidfVectorizer from scikit-learn\n2) TF-IDF stands for Term Frequency-Inverse Document Frequency.\n3) It converts text into numbers based on how important words are in each document (essay).\n4)  creating a TF-IDF vectorizer object.\n5) max_features=1000: Keep only the top 1000 most important words.\n6) stop_words='english': Ignore very common English words (like the, is, and).\n7) `X = tfidf.fit_transform(df['text']) -->` Goes through every essay in df['text'] and Builds a list of important words across all essays Then, for each essay, it creates a row of numbers — how much each word matters in that essay Ex: Essay 1: \"Cars are fast and useful.\"\n→ Might become [0.6, 0.4, 0.5, 0, 0, 0]\n8) X is now a feature matrix\nX.shape:  (number of essays, number of words), each row is one essay and each column is one word\n9) TF-IDF know identifies words as important:\n    Term Frequency (TF): How often a word appears in one essay, more appearance = more importance.\n    Inverse Document Frequency (IDF): How rare that word is across all essays, 'the' is the least rare so least score.\n10) And then the model will identify an essay as ai generated or not through:\n    The model doesn’t \"understand\" meaning like a human.\n    But it learns patterns in word usage EX: Students often write: \"I think...\", \"in conclusion...\", \"my opinion is...\"\n    AI often writes: \"This essay aims to...\", \"one might argue that...\", \"in recent years...\"\n    Even if both sound good, their word patterns are different.\n    TF-IDF captures those patterns as numbers.\n    Your model (like SVM or a neural net) looks at those numbers and learns the difference from training data.\n    It doesn’t know “why” — it just sees that essays with pattern A → usually label 0 (student),\n    and essays with pattern B → usually label 1 (AI).\n","metadata":{}},{"cell_type":"markdown","source":"Splitting the DS","metadata":{}},{"cell_type":"code","source":"#clean the text data to prevent using irrelevant words or nonmeaningful ones\nimport re\n\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove numbers and punctuation\n    text = re.sub(r'[^a-z\\s]', '', text)\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\nx = textDetection_df['text']\ny = textDetection_df['generated']\n# Splitting the dataset into training and testing sets\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n# Apply cleaning to all text entries\n# Convert x_train and x_test to DataFrames to add a new column\nx_train = x_train.to_frame()\nx_test = x_test.to_frame()\n\n# Apply cleaning to all text entries\nx_train['clean_text'] = x_train['text'].apply(clean_text)\nx_test['clean_text'] = x_test['text'].apply(clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:25.424239Z","iopub.execute_input":"2025-09-07T23:33:25.424542Z","iopub.status.idle":"2025-09-07T23:33:30.534576Z","shell.execute_reply.started":"2025-09-07T23:33:25.42452Z","shell.execute_reply":"2025-09-07T23:33:30.533274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Getting features","metadata":{}},{"cell_type":"code","source":"# Converting text to features using TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ntfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', min_df=5, max_df=0.9)# only keep words that appear in 5+ docs & ignore words that appear in 90%+ of docs\nfeatures_from_text_train = tfidf_vectorizer.fit_transform(x_train['clean_text']).toarray()\nfeatures_from_text_test = tfidf_vectorizer.transform(x_test['clean_text']).toarray()#features_from_text is now a feature matrix\n#features_from_text.shape:  (number of essays, number of words), each row is one essay and each column is one word\n\nprint(\"Non-zero values:\", np.count_nonzero(features_from_text_train))\nprint(\"Shape:\", features_from_text_train.shape)\n# After fitting the vectorizer\nfeature_names = tfidf_vectorizer.get_feature_names_out()\nprint(feature_names[:100])  # print first 20 words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:30.535462Z","iopub.execute_input":"2025-09-07T23:33:30.535804Z","iopub.status.idle":"2025-09-07T23:33:37.417765Z","shell.execute_reply.started":"2025-09-07T23:33:30.535775Z","shell.execute_reply":"2025-09-07T23:33:37.416669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_from_text_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:37.418697Z","iopub.execute_input":"2025-09-07T23:33:37.41897Z","iopub.status.idle":"2025-09-07T23:33:37.425937Z","shell.execute_reply.started":"2025-09-07T23:33:37.418949Z","shell.execute_reply":"2025-09-07T23:33:37.424964Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Scaling through standardscaler & Feature Reduction with PCA:\n","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nx_traScal = scaler.fit_transform(features_from_text_train)\nx_tesScal = scaler.transform(features_from_text_test)\n#########################################################################################\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=100)\nx_tra_pca = pca.fit_transform(x_traScal)\nx_tes_pca = pca.transform(x_tesScal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:37.427Z","iopub.execute_input":"2025-09-07T23:33:37.427343Z","iopub.status.idle":"2025-09-07T23:33:40.220891Z","shell.execute_reply.started":"2025-09-07T23:33:37.427314Z","shell.execute_reply":"2025-09-07T23:33:40.219917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Showing diff correlations bet data:","metadata":{}},{"cell_type":"code","source":"#converting features_from_text matrix to a DataFrame to see each word and its corresponding TF-IDF value score of importance\nimport pandas as pd\n\nfeatures_from_textdf = pd.DataFrame(features_from_text_train, columns=feature_names)\nprint(features_from_textdf.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:40.221931Z","iopub.execute_input":"2025-09-07T23:33:40.222208Z","iopub.status.idle":"2025-09-07T23:33:40.237527Z","shell.execute_reply.started":"2025-09-07T23:33:40.222188Z","shell.execute_reply":"2025-09-07T23:33:40.236491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Showing correlation between the features' words\ntop10words = features_from_textdf.mean().sort_values(ascending=False).head(10).index.tolist()\ncorrelation_matrix1 = features_from_textdf[top10words].corr()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix1, cmap='coolwarm', annot=True)\nplt.title(\"Correlation Between TF-IDF Features\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:33:40.238587Z","iopub.execute_input":"2025-09-07T23:33:40.238893Z","iopub.status.idle":"2025-09-07T23:33:40.817959Z","shell.execute_reply.started":"2025-09-07T23:33:40.238844Z","shell.execute_reply":"2025-09-07T23:33:40.817035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Displaying Top Important Words by Average TF-IDF Score","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\ntop_20_words = features_from_textdf.mean().sort_values(ascending=False).head(20)\n\nfig = px.bar(\n    x=top_20_words.values,\n    y=top_20_words.index,\n    labels={'x': 'Avg TF-IDF Score', 'y': 'Word'},\n    title='Top 20 Most Important Words (by Average TF-IDF Score)',\n    color=top_20_words.values,\n    color_continuous_scale='Viridis'\n)\n\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:34:45.190287Z","iopub.execute_input":"2025-09-07T23:34:45.190551Z","iopub.status.idle":"2025-09-07T23:34:45.33595Z","shell.execute_reply.started":"2025-09-07T23:34:45.190531Z","shell.execute_reply":"2025-09-07T23:34:45.33464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SVM Models","metadata":{}},{"cell_type":"markdown","source":"4) Modeling\n A-Using SVM for classification + model evaluation","metadata":{}},{"cell_type":"code","source":"svm = { 'linear svm' : SVC(kernel='linear') ,\n       'svc (RBF kernel)': SVC(kernel='rbf') }\n\nprint('# SVM MODELS # \\n')\nfor name , model in svm.items():\n    model.fit(x_tra_pca,y_train)\n    y_pred = model.predict(x_tes_pca)\n    print('Accuracy of model ' , name , accuracy_score(y_test,y_pred) , '\\n')\n    print(classification_report(y_test,y_pred))\n    print('########################################################\\n')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:34:13.058212Z","iopub.execute_input":"2025-09-07T23:34:13.058541Z","iopub.status.idle":"2025-09-07T23:34:45.188696Z","shell.execute_reply.started":"2025-09-07T23:34:13.058522Z","shell.execute_reply":"2025-09-07T23:34:45.187354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"poly_svm = SVC(kernel='poly', degree=3)\npoly_svm.fit(x_tra_pca, y_train)\n\n#svm polynomial model\ny_pred_poly = poly_svm.predict(x_tes_pca)\nprint('Accuracy of model ' , 'Polynomial SVM' , accuracy_score(y_test,y_pred_poly) , '\\n')\nprint(classification_report(y_test, y_pred_poly))\nprint('########################################################\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:35:11.995494Z","iopub.execute_input":"2025-09-07T23:35:11.995795Z","iopub.status.idle":"2025-09-07T23:35:20.533728Z","shell.execute_reply.started":"2025-09-07T23:35:11.995774Z","shell.execute_reply":"2025-09-07T23:35:20.532396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lin_model = SVC(kernel='linear')\nlin_model.fit(x_tra_pca,y_train)\ny_pred_lin = lin_model.predict(x_tes_pca)\n\nconf_lin = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_lin))   \nconf_lin.plot()\nplt.title('Confusion Matrix for Linear SVM')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:35:20.535364Z","iopub.execute_input":"2025-09-07T23:35:20.535658Z","iopub.status.idle":"2025-09-07T23:35:45.110061Z","shell.execute_reply.started":"2025-09-07T23:35:20.535631Z","shell.execute_reply":"2025-09-07T23:35:45.109025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_poly = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_poly))\nconf_poly.plot()\nplt.title('Confusion Matrix for Polynomial SVM')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:35:45.111162Z","iopub.execute_input":"2025-09-07T23:35:45.111416Z","iopub.status.idle":"2025-09-07T23:35:45.435329Z","shell.execute_reply.started":"2025-09-07T23:35:45.111398Z","shell.execute_reply":"2025-09-07T23:35:45.434159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RBF_model = SVC(kernel='rbf')\nRBF_model.fit(x_tra_pca,y_train)\ny_pred_RBF = RBF_model.predict(x_tes_pca)\n\nconf_RBF = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_RBF))\nconf_RBF.plot()\nplt.title('Confusion Matrix for RBF SVM')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:35:45.436777Z","iopub.execute_input":"2025-09-07T23:35:45.437068Z","iopub.status.idle":"2025-09-07T23:35:51.743337Z","shell.execute_reply.started":"2025-09-07T23:35:45.437046Z","shell.execute_reply":"2025-09-07T23:35:51.742248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neural Networks Model","metadata":{}},{"cell_type":"markdown","source":"Neural Network Implementation ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport joblib\n\n# Step 1: Define hyperparameter grid\nparam_grid = {\n    'hidden_layer_sizes': [(10,), (50,), (64, 32), (64, 32, 16)],\n    'activation': ['relu', 'logistic'],\n    'alpha': [0.0001, 0.001, 0.01],\n    'learning_rate_init': [0.001, 0.01]\n}\n\n# Step 2: Define base model\nbase_mlp = MLPClassifier(max_iter=200, early_stopping=True, random_state=42)\n\n# Step 3: Grid search\ngrid_search = GridSearchCV(\n    base_mlp, param_grid, cv=5,\n    n_jobs=-1, verbose=1, scoring='accuracy'\n)\ngrid_search.fit(x_tra_pca, y_train)\n\n# Step 4: Get the best model\nbest_model = grid_search.best_estimator_\nprint(\"\\n Best Hyperparameters:\")\nprint(grid_search.best_params_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:35:54.750807Z","iopub.execute_input":"2025-09-07T23:35:54.751167Z","iopub.status.idle":"2025-09-07T23:39:51.428317Z","shell.execute_reply.started":"2025-09-07T23:35:54.751141Z","shell.execute_reply":"2025-09-07T23:39:51.427458Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"the Neural Network Loss Curve, shows how the loss (error) of your model changes over training epochs (iterations).","metadata":{}},{"cell_type":"code","source":"# Step 5: Evaluate on test set\ny_predNN = best_model.predict(x_tes_pca)\nacc = accuracy_score(y_test, y_predNN)\nprint(f\"\\nTest Accuracy: {acc:.4f}\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_predNN))\n\n# Step 6: Cross-validation on training set\ncv_scores = cross_val_score(best_model, x_tra_pca, y_train, cv=5, scoring='accuracy')\nprint(f\"Cross-validation scores (5-fold): {cv_scores}\")\nprint(f\"Mean CV Accuracy: {cv_scores.mean():.4f}\")\n\n# Step 7: Plot loss curve\nplt.plot(best_model.loss_curve_)\nplt.title(\"Neural Network Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.grid(True)\nplt.show()\n\n# Step 8: Save the best model\njoblib.dump(best_model, \"best_nn_model.pkl\")\nprint(\" Best model saved as 'best_nn_model.pkl'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:39:57.65087Z","iopub.execute_input":"2025-09-07T23:39:57.651239Z","iopub.status.idle":"2025-09-07T23:40:03.884849Z","shell.execute_reply.started":"2025-09-07T23:39:57.651206Z","shell.execute_reply":"2025-09-07T23:40:03.883794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_nn = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_predNN))   \nconf_nn.plot()\nplt.title('Confusion Matrix for NN')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:43:01.427378Z","iopub.execute_input":"2025-09-07T23:43:01.427699Z","iopub.status.idle":"2025-09-07T23:43:01.601199Z","shell.execute_reply.started":"2025-09-07T23:43:01.427676Z","shell.execute_reply":"2025-09-07T23:43:01.600245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Evaluation","metadata":{}},{"cell_type":"code","source":"print(\"Model Comparison Summary:\")\nsummary_data = {\n    \"Model\": [\"SVM-Linear\", \"SVM-RBF\", \"SVM-Poly\", \"Neural Net (Best)\"],\n    \"Test Accuracy\": [accuracy_score(y_test, y_pred_lin),\n                      accuracy_score(y_test, y_pred_RBF),\n                      accuracy_score(y_test, y_pred_poly),\n                      accuracy_score(y_test, y_predNN)],\n}\nsummary_df = pd.DataFrame(summary_data)\nprint(summary_df)\nbest_model_name = summary_df.loc[summary_df['Test Accuracy'].idxmax(), 'Model']\nprint(f'Best Model is: {best_model_name}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T23:43:04.750975Z","iopub.execute_input":"2025-09-07T23:43:04.751317Z","iopub.status.idle":"2025-09-07T23:43:04.767068Z","shell.execute_reply.started":"2025-09-07T23:43:04.751263Z","shell.execute_reply":"2025-09-07T23:43:04.766052Z"}},"outputs":[],"execution_count":null}]}